{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13741047,"datasetId":8743078}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DA5401-2025-Data-Challenge\n\n## By : B.S.Tejas","metadata":{}},{"cell_type":"code","source":"import os, json, random\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.auto import tqdm\n\nfrom sentence_transformers import SentenceTransformer\n\n# ------------------ SEED ------------------\nRND = 42\nrandom.seed(RND); np.random.seed(RND)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ------------------ HUGGINGFACE LOGIN ------------------\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\nexcept Exception:\n    print(\"Kaggle Secrets not available. Checking HF_TOKEN env var...\")\n    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n\nif HF_TOKEN:\n    from huggingface_hub import login\n    login(token=HF_TOKEN, add_to_git_credential=False)\n    print(\"Successfully logged into Hugging Face.\")\nelse:\n    print(\"HF_TOKEN not found. If Gemma is gated, this may fail.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:46:10.603837Z","iopub.execute_input":"2025-11-18T16:46:10.604433Z","iopub.status.idle":"2025-11-18T16:46:10.906086Z","shell.execute_reply.started":"2025-11-18T16:46:10.604404Z","shell.execute_reply":"2025-11-18T16:46:10.905299Z"}},"outputs":[{"name":"stdout","text":"Successfully logged into Hugging Face.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Dataset Loadin & Feature Engineering","metadata":{}},{"cell_type":"code","source":"# ------------------ CONFIG ------------------\nKAGGLE_DIR = \"/kaggle/input/da5401-dataset\"\nTRAIN_JSON = os.path.join(KAGGLE_DIR, \"train_data.json\")\nTEST_JSON  = os.path.join(KAGGLE_DIR, \"test_data.json\")\nMETRIC_NAMES = os.path.join(KAGGLE_DIR, \"metric_names.json\")\nMETRIC_EMBS  = os.path.join(KAGGLE_DIR, \"metric_name_embeddings.npy\")\nSUBMISSION_FILE = os.path.join(KAGGLE_DIR, \"sample_submission.csv\")\n\nOUT_TRAIN = \"/kaggle/working/train_oof_meta_learn.csv\"\nOUT_AUG   = \"/kaggle/working/train_aug_meta_learn.csv\"\nOUT_COMB  = \"/kaggle/working/combined_oof_meta_learn.csv\"\nOUT_SUB   = \"/kaggle/working/submission_da5401_meta_learn_bias.csv\"\n\n# tuned hyperparams\nNEG_PER_SAMPLE   = 1\nSVD_DIM          = 145\nTFIDF_MAX_FEAT   = 20000\nN_SPLITS         = 5\nRIDGE_ALPHA      = 1.0          # base Ridge\nRF_N_ESTIMATORS  = 1000         # tuned RF size\nRF_MAX_DEPTH     = None         # tuned (None = unlimited)\nRF_MIN_LEAF      = 1            # tuned\nMETA_ALPHA       = 1          # tuned meta-learner Ridge\nBIAS_ALPHA       = 3.0          # tuned bias smoothing\n\n# ------------------ SANITY CHECK ------------------\nfor p in [TRAIN_JSON, METRIC_NAMES, METRIC_EMBS, TEST_JSON, SUBMISSION_FILE]:\n    if not os.path.exists(p):\n        raise FileNotFoundError(f\"Required file not found: {p}\")\n\n# ------------------ LOAD TRAIN DATA ------------------\nprint(\"Loading raw json files...\")\nwith open(TRAIN_JSON, \"r\", encoding=\"utf-8\") as f:\n    train_raw = json.load(f)\nwith open(METRIC_NAMES, \"r\", encoding=\"utf-8\") as f:\n    metric_names = json.load(f)\n\nmetric_map = {name: i for i, name in enumerate(metric_names)}\nmetric_embs = np.load(METRIC_EMBS)   # (145, 768)\nprint(\"Metric embeddings shape:\", metric_embs.shape)\n\nrows = []\nfor rec in train_raw:\n    rows.append({\n        \"metric_name\": rec.get(\"metric_name\"),\n        \"metric_idx\": metric_map.get(rec.get(\"metric_name\"), -1),\n        \"prompt\": (rec.get(\"prompt\",\"\") or \"\"),\n        \"system_prompt\": (rec.get(\"system_prompt\",\"\") or \"\"),\n        \"response\": (rec.get(\"response\",\"\") or \"\"),\n        \"score\": float(rec.get(\"score\", 0.0))\n    })\ndf = pd.DataFrame(rows)\nprint(\"Train rows:\", len(df))\n\n# ------------------ TEXT CONCAT ------------------\ncombined_texts = (df[\"prompt\"].fillna(\"\") + \" [SEP] \" +\n                  df[\"system_prompt\"].fillna(\"\") + \" [SEP] \" +\n                  df[\"response\"].fillna(\"\")).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:46:14.641482Z","iopub.execute_input":"2025-11-18T16:46:14.642203Z","iopub.status.idle":"2025-11-18T16:46:15.072409Z","shell.execute_reply.started":"2025-11-18T16:46:14.642181Z","shell.execute_reply":"2025-11-18T16:46:15.071585Z"}},"outputs":[{"name":"stdout","text":"Loading raw json files...\nMetric embeddings shape: (145, 768)\nTrain rows: 5000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# ------------------ TF-IDF + SVD ------------------\nprint(\"\\n=== Building TF-IDF & SVD text features ===\")\ntfidf = TfidfVectorizer(max_features=TFIDF_MAX_FEAT, ngram_range=(1,2))\nX_tfidf = tfidf.fit_transform(combined_texts)\nprint(\"TF-IDF shape:\", X_tfidf.shape)\n\nsvd = TruncatedSVD(n_components=SVD_DIM, random_state=RND)\nX_text_svd = svd.fit_transform(X_tfidf)\nprint(\"Text SVD shape:\", X_text_svd.shape)\n\n# ------------------ METRIC SVD ------------------\nprint(\"Reducing metric embeddings via SVD...\")\nsvd_metric = TruncatedSVD(n_components=SVD_DIM, random_state=RND)\nmetric_embs_reduced = svd_metric.fit_transform(metric_embs)\nprint(\"Metric reduced shape:\", metric_embs_reduced.shape)\n\nmetric_vecs = np.vstack([\n    metric_embs_reduced[idx] if (0 <= idx < metric_embs_reduced.shape[0]) else np.zeros(SVD_DIM)\n    for idx in df[\"metric_idx\"].values\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:46:20.627171Z","iopub.execute_input":"2025-11-18T16:46:20.627727Z","iopub.status.idle":"2025-11-18T16:46:25.088879Z","shell.execute_reply.started":"2025-11-18T16:46:20.627701Z","shell.execute_reply":"2025-11-18T16:46:25.087324Z"}},"outputs":[{"name":"stdout","text":"\n=== Building TF-IDF & SVD text features ===\nTF-IDF shape: (5000, 20000)\nText SVD shape: (5000, 145)\nReducing metric embeddings via SVD...\nMetric reduced shape: (145, 145)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\n# ------------------ PAIRWISE FEATS ------------------\ndef pairwise_feats(A, B):\n    assert A.shape == B.shape, f\"pairwise_feats: shape mismatch {A.shape} vs {B.shape}\"\n    dot = np.sum(A * B, axis=1)\n    an  = np.linalg.norm(A, axis=1)\n    bn  = np.linalg.norm(B, axis=1)\n    cos = dot / (an * bn + 1e-9)\n    l1  = np.sum(np.abs(A - B), axis=1)\n    l2  = np.sqrt(np.sum((A - B)**2, axis=1))\n    prod = A * B\n    return {\n        \"dot\": dot, \"cos\": cos,\n        \"l1\": l1, \"l2\": l2,\n        \"prod_mean\": prod.mean(axis=1),\n        \"prod_std\":  prod.std(axis=1),\n        \"ad_mean\": np.abs(A - B).mean(axis=1),\n        \"ad_std\":  np.abs(A - B).std(axis=1)\n    }\n\nprint(\"\\n=== metric-vs-text pairwise features (SVD space) ===\")\nmetric_pairs = pairwise_feats(metric_vecs, X_text_svd)\nfeat_df = pd.DataFrame(metric_pairs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:46:35.840875Z","iopub.execute_input":"2025-11-18T16:46:35.841456Z","iopub.status.idle":"2025-11-18T16:46:35.875052Z","shell.execute_reply.started":"2025-11-18T16:46:35.841419Z","shell.execute_reply":"2025-11-18T16:46:35.874392Z"}},"outputs":[{"name":"stdout","text":"\n=== metric-vs-text pairwise features (SVD space) ===\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ------------------ PROTOTYPES IN TEXT SVD SPACE ------------------\nprint(\"Computing prototypes per metric in text SVD space...\")\nproto_by_metric = {}\nfor mid, g in df.groupby(\"metric_idx\"):\n    idxs = g.index.values\n    if len(idxs) > 0:\n        proto_by_metric[int(mid)] = X_text_svd[idxs].mean(axis=0)\n    else:\n        proto_by_metric[int(mid)] = np.zeros(SVD_DIM)\n\nn_metrics = metric_embs_reduced.shape[0]\nproto_matrix = np.vstack([proto_by_metric.get(i, np.zeros(SVD_DIM)) for i in range(n_metrics)])\nproto_mat_self = np.vstack([proto_by_metric.get(int(mid), np.zeros(SVD_DIM))\n                            for mid in df[\"metric_idx\"].values])\n\nproto_pairs = pairwise_feats(proto_mat_self, X_text_svd)\nfor k, v in proto_pairs.items():\n    feat_df[f\"{k}_pt\"] = v\n\n# ------------------ PROTOTYPE MARGIN ------------------\nprint(\"Prototype margin (cosine to all prototypes) ...\")\nP = proto_matrix\nP_norm = np.linalg.norm(P, axis=1) + 1e-9\nT = X_text_svd\nT_norm = np.linalg.norm(T, axis=1) + 1e-9\ncos_mat = (T @ P.T) / (T_norm[:, None] * P_norm[None, :])\n\nmetric_idx_arr = df[\"metric_idx\"].values.astype(int)\nrow_idx = np.arange(len(df))\nself_cos = cos_mat[row_idx, metric_idx_arr]\ncos_mat_wo = cos_mat.copy()\ncos_mat_wo[row_idx, metric_idx_arr] = -1e9\nbest_other = cos_mat_wo.max(axis=1)\nmargin = self_cos - best_other\n\nfeat_df[\"cos_pt_self_full\"] = self_cos\nfeat_df[\"cos_pt_other_max\"] = best_other\nfeat_df[\"cos_pt_margin\"]    = margin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:46:49.210360Z","iopub.execute_input":"2025-11-18T16:46:49.211069Z","iopub.status.idle":"2025-11-18T16:46:49.290291Z","shell.execute_reply.started":"2025-11-18T16:46:49.211038Z","shell.execute_reply":"2025-11-18T16:46:49.289426Z"}},"outputs":[{"name":"stdout","text":"Computing prototypes per metric in text SVD space...\nPrototype margin (cosine to all prototypes) ...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ------------------ GEMMA TEXT ENCODING ------------------\nprint(\"\\n=== Encoding with Gemma (google/embeddinggemma-300m) for text ===\")\ngemma_model = SentenceTransformer(\"google/embeddinggemma-300m\")\nX_text_gemma = gemma_model.encode(\n    combined_texts,\n    batch_size=64,\n    show_progress_bar=True,\n    convert_to_numpy=True\n)\nprint(\"Gemma embeddings shape:\", X_text_gemma.shape)\n\nG = metric_embs              # (145, 768) in same space as Gemma\nG_norm = np.linalg.norm(G, axis=1) + 1e-9\nTg = X_text_gemma            # (5000, 768)\nTg_norm = np.linalg.norm(Tg, axis=1) + 1e-9\ncos_gemma = (Tg @ G.T) / (Tg_norm[:, None] * G_norm[None, :])\n\nself_cos_g = cos_gemma[row_idx, metric_idx_arr]\ncos_g_wo = cos_gemma.copy()\ncos_g_wo[row_idx, metric_idx_arr] = -1e9\nbest_other_g = cos_g_wo.max(axis=1)\nmargin_g = self_cos_g - best_other_g\n\nfeat_df[\"gm_cos_self\"]      = self_cos_g\nfeat_df[\"gm_cos_other_max\"] = best_other_g\nfeat_df[\"gm_cos_margin\"]    = margin_g\n\n# ------------------ SIMPLE SCALARS + LANG FLAGS ------------------\nfeat_df[\"len_prompt\"]   = df[\"prompt\"].apply(lambda x: len(x or \"\"))\nfeat_df[\"len_response\"] = df[\"response\"].apply(lambda x: len(x or \"\"))\nfeat_df[\"ratio_resp_pr\"] = (feat_df[\"len_response\"] + 1) / (feat_df[\"len_prompt\"] + 1)\n\ndef lang_flag(s):\n    if any(\"\\u0B80\" <= ch <= \"\\u0BFF\" for ch in s): return \"ta_te\"\n    if any(\"\\u0900\" <= ch <= \"\\u097F\" for ch in s): return \"hi\"\n    if any(\"\\u0980\" <= ch <= \"\\u09FF\" for ch in s): return \"bn\"\n    return \"latin\"\n\nlangs = (df[\"prompt\"].fillna(\"\") + \" \" + df[\"response\"].fillna(\"\")).apply(lang_flag)\nfeat_df[\"lang_is_latin\"] = (langs == \"latin\").astype(int)\nfeat_df[\"lang_is_hi\"]    = (langs == \"hi\").astype(int)\nfeat_df[\"lang_is_ta_te\"] = (langs == \"ta_te\").astype(int)\n\nfeat_df[\"metric_idx\"] = df[\"metric_idx\"].astype(int)\nfeat_df[\"score\"]      = df[\"score\"].astype(float)\n\nmetric_te = feat_df.groupby(\"metric_idx\")[\"score\"].mean().to_dict()\nfeat_df[\"metric_te\"] = feat_df[\"metric_idx\"].map(metric_te).fillna(feat_df[\"score\"].mean())\n\nfeat_df.to_csv(OUT_TRAIN, index=False)\nprint(\"Saved base train features to:\", OUT_TRAIN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:47:12.464461Z","iopub.execute_input":"2025-11-18T16:47:12.465101Z","iopub.status.idle":"2025-11-18T16:49:12.619695Z","shell.execute_reply.started":"2025-11-18T16:47:12.465076Z","shell.execute_reply":"2025-11-18T16:49:12.618851Z"}},"outputs":[{"name":"stdout","text":"\n=== Encoding with Gemma (google/embeddinggemma-300m) for text ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/573 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58f50227687e4cc4bb5047ebe960233c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/997 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c50b9d2f4624e408eb8fcb8dfffd12f"}},"metadata":{}},{"name":"stderr","text":"WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.1.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/18.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb968f1ea9da4a57a5b80cc0d94b0b6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/58.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3897a93b925447d82a88ed84d5c2fc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"134aa0a816ea43c4a4fc338b9eaeb303"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48ef7ff3ae7478892369903c546bf56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fd8fce7fcfd454d967d17d3c28d3154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"574390a339c4413d97ab9af23f96cb78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a99b7507fc444852ae2fe0ef25987d58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"324f42051e9549b08902a88c147b6d31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"195aab5dc92c4b97bb02d2135e3eb4f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/312 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a89d10d0fe24dd18c8d729da7ffa3a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63cbe48d97e4405da12ef793aa6e36ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"2_Dense/model.safetensors:   0%|          | 0.00/9.44M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d5fb242a3f46f49b0fde7a0dc1e5ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3_Dense/model.safetensors:   0%|          | 0.00/9.44M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8928a96f6d134de58627416de4bd2d3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50034db49c8a471ba1ea7d3dcb82a9b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/79 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19530e1ffae46918591144747f15c88"}},"metadata":{}},{"name":"stdout","text":"Gemma embeddings shape: (5000, 768)\nSaved base train features to: /kaggle/working/train_oof_meta_learn.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\n# ------------------ AUGMENTATION (NEGATIVE SAMPLES) ------------------\nprint(\"\\n=== Generating augmented rows (NEG_PER_SAMPLE = %d ) ===\" % NEG_PER_SAMPLE)\nunique_metrics = df[\"metric_idx\"].unique().tolist()\naug_rows = []\nfor i in tqdm(range(len(feat_df)), desc=\"Creating aug rows\"):\n    base = feat_df.iloc[i].to_dict()\n    base[\"is_synth\"] = False\n    aug_rows.append(base)\n\n    cos_row_proto = cos_mat[i].copy()\n    cos_row_g     = cos_gemma[i].copy()\n    for _ in range(NEG_PER_SAMPLE):\n        wrong = int(base[\"metric_idx\"])\n        while wrong == base[\"metric_idx\"]:\n            wrong = int(random.choice(unique_metrics))\n\n        wrong_vec  = metric_embs_reduced[wrong]\n        text_vec   = X_text_svd[i]\n        pf         = pairwise_feats(wrong_vec.reshape(1,-1), text_vec.reshape(1,-1))\n        wrong_proto = proto_by_metric.get(wrong, np.zeros(SVD_DIM))\n        pf_pt       = pairwise_feats(wrong_proto.reshape(1,-1), text_vec.reshape(1,-1))\n\n        # prototype margin for wrong metric\n        self_cos_w = cos_row_proto[wrong]\n        cos_row_proto_wo = cos_row_proto.copy()\n        cos_row_proto_wo[wrong] = -1e9\n        best_other_w = cos_row_proto_wo.max()\n        margin_w     = self_cos_w - best_other_w\n\n        # Gemma margin for wrong metric\n        self_cos_g_w = cos_row_g[wrong]\n        cos_row_g_wo = cos_row_g.copy()\n        cos_row_g_wo[wrong] = -1e9\n        best_other_g_w = cos_row_g_wo.max()\n        margin_g_w     = self_cos_g_w - best_other_g_w\n\n        neg = {\n            \"dot\": float(pf[\"dot\"][0]),\n            \"cos\": float(pf[\"cos\"][0]),\n            \"l1\":  float(pf[\"l1\"][0]),\n            \"l2\":  float(pf[\"l2\"][0]),\n            \"prod_mean\": float(pf[\"prod_mean\"][0]),\n            \"prod_std\":  float(pf[\"prod_std\"][0]),\n            \"ad_mean\":   float(pf[\"ad_mean\"][0]),\n            \"ad_std\":    float(pf[\"ad_std\"][0]),\n\n            \"dot_pt\": float(pf_pt[\"dot\"][0]),\n            \"cos_pt\": float(pf_pt[\"cos\"][0]),\n            \"l1_pt\":  float(pf_pt[\"l1\"][0]),\n            \"l2_pt\":  float(pf_pt[\"l2\"][0]),\n            \"prod_mean_pt\": float(pf_pt[\"prod_mean\"][0]),\n            \"prod_std_pt\":  float(pf_pt[\"prod_std\"][0]),\n            \"ad_mean_pt\":   float(pf_pt[\"ad_mean\"][0]),\n            \"ad_std_pt\":    float(pf_pt[\"ad_std\"][0]),\n\n            \"cos_pt_self_full\": float(self_cos_w),\n            \"cos_pt_other_max\": float(best_other_w),\n            \"cos_pt_margin\":    float(margin_w),\n\n            \"gm_cos_self\":      float(self_cos_g_w),\n            \"gm_cos_other_max\": float(best_other_g_w),\n            \"gm_cos_margin\":    float(margin_g_w),\n\n            \"len_prompt\":   float(base[\"len_prompt\"]),\n            \"len_response\": float(base[\"len_response\"]),\n            \"ratio_resp_pr\":float(base[\"ratio_resp_pr\"]),\n            \"lang_is_latin\":int(base[\"lang_is_latin\"]),\n            \"lang_is_hi\":   int(base[\"lang_is_hi\"]),\n            \"lang_is_ta_te\":int(base[\"lang_is_ta_te\"]),\n            \"metric_idx\":   int(wrong),\n            \"score\":        float(random.choice([0,1,2])),\n            \"metric_te\":    float(metric_te.get(wrong, feat_df[\"score\"].mean())),\n            \"is_synth\": True\n        }\n        aug_rows.append(neg)\n\naug_df = pd.DataFrame(aug_rows)\naug_df.to_csv(OUT_AUG, index=False)\nprint(\"Saved augmented dataframe to:\", OUT_AUG, \"shape:\", aug_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:49:12.620788Z","iopub.execute_input":"2025-11-18T16:49:12.621002Z","iopub.status.idle":"2025-11-18T16:49:15.685099Z","shell.execute_reply.started":"2025-11-18T16:49:12.620971Z","shell.execute_reply":"2025-11-18T16:49:15.684288Z"}},"outputs":[{"name":"stdout","text":"\n=== Generating augmented rows (NEG_PER_SAMPLE = 1 ) ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Creating aug rows:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c4a8f69eaa549fbb53afde2004fb14e"}},"metadata":{}},{"name":"stdout","text":"Saved augmented dataframe to: /kaggle/working/train_aug_meta_learn.csv shape: (10000, 32)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ------------------ FEATURE COLS ------------------\nfeature_cols = [\n    \"dot\",\"cos\",\"l1\",\"l2\",\"prod_mean\",\"prod_std\",\"ad_mean\",\"ad_std\",\n    \"dot_pt\",\"cos_pt\",\"l1_pt\",\"l2_pt\",\"prod_mean_pt\",\"prod_std_pt\",\"ad_mean_pt\",\"ad_std_pt\",\n    \"cos_pt_self_full\",\"cos_pt_other_max\",\"cos_pt_margin\",\n    \"gm_cos_self\",\"gm_cos_other_max\",\"gm_cos_margin\",\n    \"len_prompt\",\"len_response\",\"ratio_resp_pr\",\n    \"lang_is_latin\",\"lang_is_hi\",\"lang_is_ta_te\",\"metric_te\"\n]\nfeature_cols = [c for c in feature_cols if c in feat_df.columns]\nprint(\"\\nUsing feature columns:\", feature_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:49:15.685867Z","iopub.execute_input":"2025-11-18T16:49:15.686160Z","iopub.status.idle":"2025-11-18T16:49:15.691514Z","shell.execute_reply.started":"2025-11-18T16:49:15.686136Z","shell.execute_reply":"2025-11-18T16:49:15.690847Z"}},"outputs":[{"name":"stdout","text":"\nUsing feature columns: ['dot', 'cos', 'l1', 'l2', 'prod_mean', 'prod_std', 'ad_mean', 'ad_std', 'dot_pt', 'cos_pt', 'l1_pt', 'l2_pt', 'prod_mean_pt', 'prod_std_pt', 'ad_mean_pt', 'ad_std_pt', 'cos_pt_self_full', 'cos_pt_other_max', 'cos_pt_margin', 'gm_cos_self', 'gm_cos_other_max', 'gm_cos_margin', 'len_prompt', 'len_response', 'ratio_resp_pr', 'lang_is_latin', 'lang_is_hi', 'lang_is_ta_te', 'metric_te']\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Models","metadata":{}},{"cell_type":"code","source":"# ------------------ BASE MODELS (Ridge + RF) ------------------\nX = feat_df[feature_cols].fillna(0).values\ny = feat_df[\"score\"].values\ngroups = feat_df[\"metric_idx\"].values\n\ngkf = GroupKFold(n_splits=N_SPLITS)\noof_ridge = np.zeros(len(y))\noof_rf    = np.zeros(len(y))\nridges = []\nrfs    = []\n\nprint(\"\\n=== Training OOF base models... ===\")\nfor fold, (tr, val) in enumerate(gkf.split(X, y, groups)):\n    print(\" Fold\", fold)\n    r = Ridge(alpha=RIDGE_ALPHA, random_state=RND)\n    r.fit(X[tr], y[tr])\n    oof_ridge[val] = r.predict(X[val])\n    ridges.append(r)\n\n    rf = RandomForestRegressor(\n        n_estimators   = RF_N_ESTIMATORS,\n        max_depth      = RF_MAX_DEPTH,\n        min_samples_leaf = RF_MIN_LEAF,\n        n_jobs=-1,\n        random_state=RND\n    )\n    rf.fit(X[tr], y[tr])\n    oof_rf[val] = rf.predict(X[val])\n    rfs.append(rf)\n\nprint(\"Ridge OOF RMSE:\", mean_squared_error(y, oof_ridge, squared=False))\nprint(\"RF    OOF RMSE:\", mean_squared_error(y, oof_rf,    squared=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:49:15.693038Z","iopub.execute_input":"2025-11-18T16:49:15.693288Z","iopub.status.idle":"2025-11-18T16:51:39.805798Z","shell.execute_reply.started":"2025-11-18T16:49:15.693273Z","shell.execute_reply":"2025-11-18T16:51:39.804895Z"}},"outputs":[{"name":"stdout","text":"\n=== Training OOF base models... ===\n Fold 0\n Fold 1\n Fold 2\n Fold 3\n Fold 4\nRidge OOF RMSE: 0.8678186308819579\nRF    OOF RMSE: 0.9136772813864203\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ------------------ RF ON AUGMENTED DATA ------------------\nprint(\"\\nTraining augmented-RF model on augmented data …\")\nX_aug = aug_df[feature_cols].fillna(0).values\ny_aug = aug_df[\"score\"].values\nrf_aug = RandomForestRegressor(\n    n_estimators   = RF_N_ESTIMATORS,\n    max_depth      = RF_MAX_DEPTH,\n    min_samples_leaf = RF_MIN_LEAF,\n    n_jobs=-1,\n    random_state=RND\n)\nrf_aug.fit(X_aug, y_aug)\naug_preds_on_real = rf_aug.predict(feat_df[feature_cols].fillna(0).values)\n\ncombined = feat_df[[\"metric_idx\",\"score\"]].copy()\ncombined[\"oof_ridge\"]     = oof_ridge\ncombined[\"oof_rf\"]        = oof_rf\ncombined[\"oof_aug_model\"] = aug_preds_on_real\ncombined.to_csv(OUT_COMB, index=False)\nprint(\"Saved combined OOFs to:\", OUT_COMB)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:51:39.806900Z","iopub.execute_input":"2025-11-18T16:51:39.807242Z","iopub.status.idle":"2025-11-18T16:53:04.979511Z","shell.execute_reply.started":"2025-11-18T16:51:39.807216Z","shell.execute_reply":"2025-11-18T16:53:04.978753Z"}},"outputs":[{"name":"stdout","text":"\nTraining augmented-RF model on augmented data …\nSaved combined OOFs to: /kaggle/working/combined_oof_meta_learn.csv\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ------------------ META-LEARNER (Ridge on [ridge, rf, aug, metric_te]) ------------------\nmeta_X = combined[[\"oof_ridge\",\"oof_rf\",\"oof_aug_model\"]].values\nmeta_extra = combined[[\"metric_idx\",\"score\"]].copy()\nmeta_extra[\"metric_te\"] = combined[\"metric_idx\"].map(metric_te)\nmeta_X_extra = np.concatenate([meta_X, meta_extra[[\"metric_te\"]].values], axis=1)\n\nmeta_y = combined[\"score\"].values\nmeta_groups = combined[\"metric_idx\"].values\n\ngkf_meta = GroupKFold(n_splits=N_SPLITS)\nmeta_oof = np.zeros(len(meta_y))\nfrom sklearn.linear_model import Ridge as MetaRidge\nmeta_models = []\n\nprint(\"\\n=== Training meta-learner (Ridge) … ===\")\nfor fold, (tr, val) in enumerate(gkf_meta.split(meta_X_extra, meta_y, meta_groups)):\n    print(\" Meta fold\", fold)\n    m = MetaRidge(alpha=META_ALPHA, random_state=RND)\n    m.fit(meta_X_extra[tr], meta_y[tr])\n    meta_oof[val] = m.predict(meta_X_extra[val])\n    meta_models.append(m)\n\nprint(\"Meta-learner OOF RMSE (no bias):\", mean_squared_error(meta_y, meta_oof, squared=False))\n\ncombined[\"meta_oof_pred\"] = meta_oof","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:53:04.980293Z","iopub.execute_input":"2025-11-18T16:53:04.980497Z","iopub.status.idle":"2025-11-18T16:53:05.001139Z","shell.execute_reply.started":"2025-11-18T16:53:04.980474Z","shell.execute_reply":"2025-11-18T16:53:05.000366Z"}},"outputs":[{"name":"stdout","text":"\n=== Training meta-learner (Ridge) … ===\n Meta fold 0\n Meta fold 1\n Meta fold 2\n Meta fold 3\n Meta fold 4\nMeta-learner OOF RMSE (no bias): 0.6067000760669571\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ------------------ BIAS CORRECTION ON META PRED ------------------\nprint(\"\\n=== Bias correction (metric-wise) ===\")\nms = combined.groupby(\"metric_idx\").agg(\n    n=(\"score\",\"size\"),\n    mean_true=(\"score\",\"mean\"),\n    mean_pred=(\"meta_oof_pred\",\"mean\")\n).reset_index()\nms[\"bias_raw\"] = ms[\"mean_true\"] - ms[\"mean_pred\"]\nms[\"bias_smoothed\"] = (ms[\"n\"] * ms[\"bias_raw\"]) / (ms[\"n\"] + BIAS_ALPHA)\nbias_map = ms.set_index(\"metric_idx\")[\"bias_smoothed\"].to_dict()\n\ncombined[\"bias\"] = combined[\"metric_idx\"].map(bias_map).fillna(0.0)\nmeta_oof_corr = np.clip(combined[\"meta_oof_pred\"] + combined[\"bias\"], 0.0, 10.0)\nprint(\"Meta-learner + bias OOF RMSE:\", mean_squared_error(meta_y, meta_oof_corr, squared=False))\n\n# update feat_df with OOFs (for any analysis you want)\nfeat_df[\"oof_ridge\"]   = oof_ridge\nfeat_df[\"oof_rf\"]      = oof_rf\nfeat_df[\"oof_aug_model\"] = aug_preds_on_real\nfeat_df[\"meta_oof\"]    = meta_oof\nfeat_df[\"meta_oof_corr\"] = meta_oof_corr\nfeat_df.to_csv(OUT_TRAIN, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:53:05.002094Z","iopub.execute_input":"2025-11-18T16:53:05.002388Z","iopub.status.idle":"2025-11-18T16:53:05.247165Z","shell.execute_reply.started":"2025-11-18T16:53:05.002363Z","shell.execute_reply":"2025-11-18T16:53:05.246262Z"}},"outputs":[{"name":"stdout","text":"\n=== Bias correction (metric-wise) ===\nMeta-learner + bias OOF RMSE: 0.5840728457241864\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"\n# ------------------ TEST INFERENCE ------------------\nprint(\"\\n=== Preparing test predictions … ===\")\nwith open(TEST_JSON, \"r\", encoding=\"utf-8\") as f:\n    test_raw = json.load(f)\ntest_rows = []\nfor rec in test_raw:\n    test_rows.append({\n        \"metric_idx\": metric_map.get(rec.get(\"metric_name\"), -1),\n        \"prompt\": (rec.get(\"prompt\",\"\") or \"\"),\n        \"system_prompt\": (rec.get(\"system_prompt\",\"\") or \"\"),\n        \"response\": (rec.get(\"response\",\"\") or \"\")\n    })\ntest_df = pd.DataFrame(test_rows)\ntest_texts = (test_df[\"prompt\"].fillna(\"\") + \" [SEP] \" +\n              test_df[\"system_prompt\"].fillna(\"\") + \" [SEP] \" +\n              test_df[\"response\"].fillna(\"\")).tolist()\n\n# text features\nX_test_tfidf = tfidf.transform(test_texts)\nX_test_svd   = svd.transform(X_test_tfidf)\n\ntest_metric_vecs = np.vstack([\n    metric_embs_reduced[idx] if 0 <= idx < metric_embs_reduced.shape[0] else np.zeros(SVD_DIM)\n    for idx in test_df[\"metric_idx\"].values\n])\ntest_pairs = pairwise_feats(test_metric_vecs, X_test_svd)\n\ntest_proto_self = np.vstack([\n    proto_by_metric.get(int(mid), np.zeros(SVD_DIM))\n    for mid in test_df[\"metric_idx\"].values\n])\ntest_proto_pairs = pairwise_feats(test_proto_self, X_test_svd)\n\nT_test   = X_test_svd\nTt_norm  = np.linalg.norm(T_test, axis=1) + 1e-9\ncos_mat_test = (T_test @ P.T) / (Tt_norm[:, None] * P_norm[None, :])\nmetric_idx_test = test_df[\"metric_idx\"].values.astype(int)\nrow_idx_test    = np.arange(len(test_df))\nself_cos_test   = cos_mat_test[row_idx_test, metric_idx_test]\ncos_mat_test_wo = cos_mat_test.copy()\ncos_mat_test_wo[row_idx_test, metric_idx_test] = -1e-9\nbest_other_test = cos_mat_test_wo.max(axis=1)\nmargin_test     = self_cos_test - best_other_test\n\nX_test_gemma = gemma_model.encode(\n    test_texts,\n    batch_size=64,\n    show_progress_bar=True,\n    convert_to_numpy=True\n)\nTg_test   = X_test_gemma\nTgn_test  = np.linalg.norm(Tg_test, axis=1) + 1e-9\ncos_gemma_test = (Tg_test @ G.T) / (Tgn_test[:, None] * G_norm[None, :])\nself_cos_g_test = cos_gemma_test[row_idx_test, metric_idx_test]\ncos_g_test_wo = cos_gemma_test.copy()\ncos_g_test_wo[row_idx_test, metric_idx_test] = -1e-9\nbest_other_g_test = cos_g_test_wo.max(axis=1)\nmargin_g_test     = self_cos_g_test - best_other_g_test\n\ntest_feat = pd.DataFrame(test_pairs)\nfor k, v in test_proto_pairs.items():\n    test_feat[f\"{k}_pt\"] = v\ntest_feat[\"cos_pt_self_full\"] = self_cos_test\ntest_feat[\"cos_pt_other_max\"] = best_other_test\ntest_feat[\"cos_pt_margin\"]    = margin_test\ntest_feat[\"gm_cos_self\"]      = self_cos_g_test\ntest_feat[\"gm_cos_other_max\"] = best_other_g_test\ntest_feat[\"gm_cos_margin\"]    = margin_g_test\ntest_feat[\"len_prompt\"]       = test_df[\"prompt\"].apply(lambda x: len(x or \"\"))\ntest_feat[\"len_response\"]     = test_df[\"response\"].apply(lambda x: len(x or \"\"))\ntest_feat[\"ratio_resp_pr\"]    = (test_feat[\"len_response\"] + 1)/(test_feat[\"len_prompt\"] + 1)\nlangs_test = (test_df[\"prompt\"].fillna(\"\") + \" \" + test_df[\"response\"].fillna(\"\")).apply(lang_flag)\ntest_feat[\"lang_is_latin\"] = (langs_test == \"latin\").astype(int)\ntest_feat[\"lang_is_hi\"]    = (langs_test == \"hi\").astype(int)\ntest_feat[\"lang_is_ta_te\"] = (langs_test == \"ta_te\").astype(int)\ntest_feat[\"metric_idx\"]    = test_df[\"metric_idx\"].values\ntest_feat[\"metric_te\"]     = test_feat[\"metric_idx\"].map(metric_te).fillna(feat_df[\"score\"].mean())\n\nX_test_feat = test_feat[feature_cols].fillna(0).values\ntest_ridge_preds = np.column_stack([m.predict(X_test_feat) for m in ridges]).mean(axis=1)\ntest_rf_preds    = np.column_stack([m.predict(X_test_feat) for m in rfs]).mean(axis=1)\ntest_aug_preds   = rf_aug.predict(X_test_feat)\n\nmeta_test_X = np.vstack([test_ridge_preds, test_rf_preds, test_aug_preds]).T\ntest_extra  = test_feat[\"metric_te\"].values.reshape(-1,1)\nmeta_test_X_extra = np.concatenate([meta_test_X, test_extra], axis=1)\ntest_meta_preds   = np.mean([m.predict(meta_test_X_extra) for m in meta_models], axis=0)\n\ntest_biases   = np.array([bias_map.get(int(mi), 0.0) for mi in test_df[\"metric_idx\"].values])\ntest_meta_corr = np.clip(test_meta_preds + test_biases, 0.0, 10.0)\n\nsample = pd.read_csv(SUBMISSION_FILE)\nif len(sample) == len(test_meta_corr):\n    out_sub = pd.DataFrame({\"ID\": sample.iloc[:,0].values, \"score\": test_meta_corr})\nelse:\n    out_sub = pd.DataFrame({\"ID\": np.arange(1, len(test_meta_corr)+1), \"score\": test_meta_corr})\n\nout_sub.to_csv(OUT_SUB, index=False)\nprint(\"\\nSaved submission to:\", OUT_SUB)\nprint(out_sub.head())\n\nprint(\"\\nDONE.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T16:53:05.248723Z","iopub.execute_input":"2025-11-18T16:53:05.248932Z","iopub.status.idle":"2025-11-18T16:54:23.286241Z","shell.execute_reply.started":"2025-11-18T16:53:05.248915Z","shell.execute_reply":"2025-11-18T16:54:23.285460Z"}},"outputs":[{"name":"stdout","text":"\n=== Preparing test predictions … ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49d046c3431a40cab51a6faa61091142"}},"metadata":{}},{"name":"stdout","text":"\nSaved submission to: /kaggle/working/submission_da5401_meta_learn_bias.csv\n   ID     score\n0   1  9.134753\n1   2  9.365601\n2   3  8.989640\n3   4  9.227561\n4   5  2.666913\n\nDONE.\n","output_type":"stream"}],"execution_count":17}]}